{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b38b56-c902-4ee7-bd7a-a76962a44e21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments with Berka Dataset (Classification Tasks)\n",
    "\n",
    "Supervised setting: choose stable series and label existing points as recurrent (label 1) then add noise or overlap two different series and see how well each method performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f702d3-2dc4-471f-b278-fd5f3416bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d9ee45-05ac-4a42-bc15-abb5a10a41ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose between one of these\n",
    "BASE_DATA_PATH = '../datasets/trans.asc'    # BERKA\n",
    "BASE_DATA_PATH = '../datasets/automated_transfers_bank.pqt'  # BANK_AUTOMATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba90f26-7e8d-40ba-bc80-0b8b0b8c8152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dbscanmethod\n",
    "import graphsmethod as graphmethod\n",
    "import matrixmethod\n",
    "from yousi import DetectRecurrencyII\n",
    "\n",
    "import noiser\n",
    "import swifter\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, f1_score, precision_score, recall_score,\n",
    "    recall_score, roc_curve, RocCurveDisplay, auc\n",
    ")\n",
    "\n",
    "import mercury.viz\n",
    "plt.style.use('mercury')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed8f04-6b96-484c-9adf-8bafec268a9b",
   "metadata": {},
   "source": [
    "## Read Berka Dataset and keep only stable series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5400cc49-a9d8-4468-8cdd-914e44673c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 4 # minimum length of series\n",
    "min_std = 5 # minimum std to assure stable series\n",
    "N_LAST_POINTS = 20   \n",
    "\n",
    "var_pk = 'payment_channel'\n",
    "var_date = 'date'\n",
    "var_amnt = 'amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0245aff1-ea07-4caf-b07f-904a784821b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if 'trans.asc' in BASE_DATA_PATH:\n",
    "    df = pd.read_table(BASE_DATA_PATH, sep=';', low_memory=False)\n",
    "    df = df.loc[~df.account.isna()] \n",
    "    \n",
    "    df.loc[:, 'account'] = df.loc[:, 'account'].astype(int)\n",
    "    \n",
    "    df['payment_channel'] = df.account_id.astype(str) + '-' + df.account.astype(str) +'-' + df.operation.astype(str) + '-' + df.type.astype(str)\n",
    "    df['date'] =pd.to_datetime(df['date'].astype(str), format='%y%m%d')\n",
    "\n",
    "if 'automated' in BASE_DATA_PATH:\n",
    "    df = pd.read_parquet('../datasets/bank_automated_transfers.pqt')\n",
    "    df['date'] =pd.to_datetime(df['date'].astype(str))#, format='%y%m%d')\n",
    "    df['type'] = 1  # Needed for YOUSFI method.\n",
    "    \n",
    "df[var_amnt] = df[var_amnt].abs()\n",
    "\n",
    "df = df.loc[:, [var_pk, var_date, var_amnt, 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b411b0d8-de8c-452e-b4a6-dd9930b5cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw datset: there are 476882 transactions and a total of 47677 unique payment channels\n"
     ]
    }
   ],
   "source": [
    "print(f'Raw datset: there are {df.shape[0]} transactions and a total of {df.payment_channel.nunique()} unique payment channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a5dd07-f556-4140-8533-b6f996c50a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking last 20 of each payment_channel...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Taking last {N_LAST_POINTS} of each payment_channel...\")\n",
    "df = df.groupby('payment_channel').tail(N_LAST_POINTS)#(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd81df4-b305-4350-a0c7-9c73edff504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable series: 46077, this represents a 96.64% of total no. of series\n",
      "Stable total transactions: 458148, this represents a 96.47% of total no. of transactions\n"
     ]
    }
   ],
   "source": [
    "# We assume that very \"amount-flat\" series are stable\n",
    "stds = df.groupby(var_pk)[var_amnt].std().sort_values(ascending=False).dropna()\n",
    "stable_payment_channels = set(stds[stds < min_std].index.tolist())\n",
    "df_stable = df[df[var_pk].isin(stable_payment_channels)]\n",
    "\n",
    "print(f'Stable series: {df_stable[var_pk].nunique()}, this represents a {round(df_stable[var_pk].nunique()/df[var_pk].nunique()*100,2)}% of total no. of series')\n",
    "print(f'Stable total transactions: {len(df_stable)}, this represents a {round(len(df_stable)/len(df)*100,2)}% of total no. of transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329d36c7-a8e8-4f61-b911-d2c04ebea3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing series with at least a break we lose 0 series\n"
     ]
    }
   ],
   "source": [
    "df = df_stable.copy()\n",
    "lelele = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby(var_pk)[var_date]\\\n",
    "    .diff(1).dt.days\n",
    "\n",
    "\n",
    "tdf = df.join(lelele, rsuffix='_diffdays')\n",
    "posid = tdf.groupby(['payment_channel']).cumcount().rename('group_position')\n",
    "tdf = tdf.join(posid)\n",
    "\n",
    "lelele = tdf.sort_values(['payment_channel',  'date'])\\\n",
    ".groupby(['payment_channel'])\\\n",
    ".apply(noiser.detect_breaks_wrapped)\n",
    "\n",
    "lelele = lelele.explode(['is_break', 'group_position'])\n",
    "\n",
    "have_no_breaks = lelele.groupby('payment_channel')['is_break'].max()\n",
    "have_no_breaks = set(have_no_breaks[have_no_breaks == False].index.tolist())\n",
    "df = df.loc[df.payment_channel.isin(have_no_breaks)]\n",
    "print(f\"After removing series with at least a break we lose {df_stable.payment_channel.nunique() - df.payment_channel.nunique()} series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0450f64-4cc5-4213-9207-a3d38646dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short series removed = 6161, 13.37% \n"
     ]
    }
   ],
   "source": [
    "# Remove payment channels with too few transactions\n",
    "cnts = df.groupby(var_pk)[var_pk].count().sort_values(ascending=True)\n",
    "df_filter = df.loc[df[var_pk].isin(cnts[cnts > min_length].index)]\n",
    "print(f'Short series removed = {df.payment_channel.nunique()- df_filter[var_pk].nunique()}, {round((df[var_pk].nunique()- df_filter[var_pk].nunique())/df[var_pk].nunique()*100,2)}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c9b98e-4d79-4f75-b6f5-36751bcd98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filter.copy()\n",
    "df = df.sort_values(['payment_channel','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "379ab9e5-7215-4a7b-9d80-1813ae7e6784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing diffdays...\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing diffdays...\")\n",
    "# compute diffdays\n",
    "lelele = df.sort_values(['payment_channel', 'date'])\\\n",
    "    .groupby('payment_channel')['date']\\\n",
    "    .diff(1).dt.days\\\n",
    "    .fillna(0)\\\n",
    "    .abs()\n",
    "\n",
    "df = df.join(lelele.rename('datediff'))\n",
    "\n",
    "df = df.sort_values(['payment_channel', 'date']).reset_index(drop=True)\n",
    "posid = df.groupby(['payment_channel']).cumcount().rename('group_position')\n",
    "df = df.join(posid)\n",
    "\n",
    "stds = df.groupby(['payment_channel'])['amount'].std()\n",
    "\n",
    "stable_channels = df.loc[df.payment_channel.isin(set(stds[stds < 5].index.tolist()))]\n",
    "\n",
    "\n",
    "stable_channels = stable_channels.drop_duplicates(['payment_channel', 'amount', 'date'])  # we wont consider series with several identical payments on the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c568b0ed-5ec7-49d2-9711-b804e86d9509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check there are no duplicates == True: series with same date and amount for the same payment channel\n"
     ]
    }
   ],
   "source": [
    "df_tmp = stable_channels.copy()\n",
    "df_tmp.drop_duplicates(subset=['payment_channel', 'date', 'amount'], keep='first', inplace=True, ignore_index=True)\n",
    "print(f'check there are no duplicates == {df_tmp.shape[0] == df.shape[0]}: series with same date and amount for the same payment channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d35b5d71-e2ab-4cae-8f46-5d3b8e1539dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    39916.000000\n",
       "mean         3.503883\n",
       "std          1.074926\n",
       "min          1.000000\n",
       "25%          3.000000\n",
       "50%          3.000000\n",
       "75%          4.000000\n",
       "max         20.000000\n",
       "Name: day, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_channels['day'] = stable_channels['date'].apply(lambda r:r.day)\n",
    "df_stats = stable_channels.groupby([var_pk])['day'].nunique()\n",
    "df_stats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc3b15-733e-416e-b427-07b90f76be17",
   "metadata": {},
   "source": [
    "## Experiment 1: Add new noise to stable series (setting 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8565f3d5-31e0-42c9-82d5-05037bfaeebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prob_perturbation = .05  # Probability to add a new point after each point of a serie\n",
    "n_desv_outlier = 3       # multiplier std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cc0eea4-4335-4ac7-b300-d6c50237d64a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436615, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = stable_channels.copy()\n",
    "#df = stable_channels_tmp.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9f4c20f-fc9a-47bb-bea3-33d419efc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "noised_df = noiser.add_noise_update_exp1(\n",
    "    dataframe=df,\n",
    "    col_pk=var_pk,\n",
    "    col_amnt=var_amnt,\n",
    "    col_date=var_date,\n",
    "    n_desv_outlier=n_desv_outlier,\n",
    "    prob_perturbation=prob_perturbation\n",
    ")\n",
    "\n",
    "noised_df = noised_df.copy().drop(['datediff', 'group_position'], axis=1)\n",
    "\n",
    "print(\"Computing diffdays...\")\n",
    "# compute diffdays\n",
    "lelele = noised_df.sort_values(['payment_channel', 'date'])\\\n",
    "    .groupby('payment_channel')['date']\\\n",
    "    .diff(1).dt.days\\\n",
    "    .fillna(0)\\\n",
    "    .abs()\n",
    "\n",
    "noised_df = pd.merge(noised_df, lelele.rename('datediff'), left_index=True, right_index=True, how='left')\n",
    "\n",
    "noised_df = noised_df.sort_values(['payment_channel', 'date']).reset_index(drop=True)\n",
    "posid = noised_df.groupby(['payment_channel']).cumcount().rename('group_position')\n",
    "noised_df = noised_df.join(posid)\n",
    "\n",
    "#dup_cols = ['trans_id', 'date', 'amount'] if 'trans_id' in noised_df.columns else ['date', 'amount']\n",
    "#noised_df.drop_duplicates(subset=dup_cols, keep='first', inplace=True, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aa9a9d1-80ad-4f50-be57-14fa99c988d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(noised_df) > len(df), f\"Noised DF should be larger since we added points!  ({len(noised_df)} vs {len(df)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a669ff-d0f7-45bc-87a5-b8947fa6612b",
   "metadata": {},
   "source": [
    "### Assesment Exp 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725921ef-8ac4-4a62-a4ab-15209b198f4d",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac85aca6-42cf-4130-8ff9-ab39a4614422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.878488\n",
       "0    0.121512\n",
       "Name: is_rec, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = noised_df.copy()\n",
    "\n",
    "# Save original label\n",
    "df['is_rec_orii'] =  df.is_rec\n",
    "\n",
    "df.is_rec.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2453a1c-0ef8-4f20-ac2b-be0616fb8f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67657/67657 [02:00<00:00, 561.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 4.62 s, total: 2min 7s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = dbscanmethod.main_dbscan_method(df)\n",
    "df['cluster_id_dbscan'] = df['is_rec']\n",
    "df['is_rec'] = df['is_rec_orii']\n",
    "#df = df.drop('is_rec_ori', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a865b-482b-416a-aaf0-fb25f0ea240b",
   "metadata": {},
   "source": [
    "#### Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ee3daad-38dc-4916-bf81-90dc12314fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39916/39916 [00:53<00:00, 746.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.5 s, sys: 932 ms, total: 52.4 s\n",
      "Wall time: 53.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def matrix_flag(data, use_dbscan=False):\n",
    "    dates = data[var_date].values\n",
    "    diff_days = data[var_date].diff(1).dt.days.dropna().values\n",
    "    amounts = data[var_amnt].values\n",
    "    orders = data.group_position.values\n",
    "\n",
    "    subseries = matrixmethod.main_matrix_method(diff_days, amounts, use_dbscan=use_dbscan)\n",
    "    \n",
    "    flags = np.ones(len(data)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return pd.Series({'cluster_id_matrix_binning': flags, 'group_position': orders})\n",
    "\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_flag, use_dbscan=False)\\\n",
    "    .explode(['cluster_id_matrix_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])\n",
    "\n",
    "df.loc[df.cluster_id_matrix_binning >= 0, 'cluster_id_matrix_binning'] = 1\n",
    "df.loc[df.cluster_id_matrix_binning < 0, 'cluster_id_matrix_binning'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e20c3-70a6-4d71-af3e-02913d543d55",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89da7f47-d5aa-4769-abca-50990acdfcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39916/39916 [01:29<00:00, 446.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 1.1 s, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def flag_matrix(dates, amounts):    \n",
    "    dates = np.array(dates).astype(np.datetime64)\n",
    "    amounts = np.array(amounts)\n",
    "    \n",
    "    datediffs_to_previous = np.diff(dates).astype('timedelta64[D]')\n",
    "    datediffs_to_previous = datediffs_to_previous/ np.timedelta64(1, 'D')\n",
    "    subseries = graphmethod.main_matrix_method_graphs(datediffs_to_previous, amounts, use_dbscan=False)\n",
    "\n",
    "    flags = np.ones(len(dates)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return flags\n",
    "    \n",
    "def matrix_udf_graphs(data):\n",
    "    dates = data.date.values\n",
    "    amounts = data.amount.values\n",
    "    orders = data.group_position.values\n",
    "    \n",
    "    subseries_ids = flag_matrix(dates, amounts)\n",
    "    \n",
    "    return pd.Series({'cluster_id_matrix_graph_binning': subseries_ids, 'group_position': orders})\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_udf_graphs)\\\n",
    "    .explode(['cluster_id_matrix_graph_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])\n",
    "df.loc[df.cluster_id_matrix_graph_binning >= 0, 'cluster_id_matrix_graph_binning'] = 1\n",
    "df.loc[df.cluster_id_matrix_graph_binning < 0, 'cluster_id_matrix_graph_binning'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50eeda6-4db2-4b9c-b680-97a6cbffa659",
   "metadata": {},
   "source": [
    "#### Yousfi (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5dcb53e-3602-4b95-99e8-865fdd09668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.type_col='type'\n",
    "        self.client_col= 'payment_channel'\n",
    "        self.customer_id= 'payment_channel'\n",
    "        self.time_col=  'date'\n",
    "        self.amount_col='amount'\n",
    "        self.trans_amount='amount'\n",
    "        self.trans_date=  'date'\n",
    "        self.trans_type=  'type'\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def get_fn(df):\n",
    "    dfs =  list((DetectRecurrencyII(\n",
    "                  trans_data = df,\n",
    "                  client_col= 'payment_channel',\n",
    "                  time_col=  'date',\n",
    "                  amount_col='amount',\n",
    "                  config=config\n",
    "                  )\n",
    "           )[1].values())\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # Add a cluster_id col for all dfs\n",
    "    try:\n",
    "        dfs = [dfs[i].assign(cluster_id = i).reset_index()  for i, d in enumerate(dfs)]\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "\n",
    "    if len(dfs) > 0:\n",
    "        concat_df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "        out = pd.merge(df, \n",
    "              concat_df, \n",
    "             left_on=['date', 'amount'], \n",
    "             right_on=['date', 'amount'], how='left')\n",
    "\n",
    "        dfs = out\n",
    "    else:\n",
    "        dfs = df.assign(cluster_id = -1)\n",
    "\n",
    "    return dfs.loc[:, ['date', 'amount', 'cluster_id']].reset_index(drop=True)    \n",
    "\n",
    "\n",
    "dft = df.copy()\n",
    "\n",
    "outt = dft.sort_values(['payment_channel', 'date']).groupby('payment_channel').apply(lambda x: get_fn(x))\n",
    "dft = pd.merge(dft, outt.reset_index().drop('level_1', axis=1), on=('payment_channel', 'date', 'amount'))\n",
    "dft['cluster_id'] = dft['cluster_id'].fillna(-1).astype(int)\n",
    "\n",
    "df['cluster_id_yousfi'] = dft.cluster_id\n",
    "\n",
    "df.loc[df['cluster_id_yousfi'] >= 0, 'cluster_id_yousfi'] = 1\n",
    "df.loc[df['cluster_id_yousfi'] < 0, 'cluster_id_yousfi'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f337e7-58b3-4ce2-abc3-3eda0cfb675c",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe9755a3-8c01-4b7b-aa65-a3715da89548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT FOR METHOD: cluster_id_matrix_graph_binning -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.96      0.84     63266\n",
      "           1       0.99      0.95      0.97    457392\n",
      "\n",
      "    accuracy                           0.96    520658\n",
      "   macro avg       0.87      0.96      0.91    520658\n",
      "weighted avg       0.96      0.96      0.96    520658\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_matrix_binning -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.99      0.86     63266\n",
      "           1       1.00      0.96      0.98    457392\n",
      "\n",
      "    accuracy                           0.96    520658\n",
      "   macro avg       0.88      0.97      0.92    520658\n",
      "weighted avg       0.97      0.96      0.96    520658\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_dbscan -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74     63266\n",
      "           1       0.97      0.95      0.96    457392\n",
      "\n",
      "    accuracy                           0.93    520658\n",
      "   macro avg       0.83      0.87      0.85    520658\n",
      "weighted avg       0.94      0.93      0.93    520658\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_yousfi -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.87      0.21     63266\n",
      "           1       0.88      0.13      0.22    457392\n",
      "\n",
      "    accuracy                           0.22    520658\n",
      "   macro avg       0.50      0.50      0.22    520658\n",
      "weighted avg       0.79      0.22      0.22    520658\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for method in ('cluster_id_matrix_graph_binning', 'cluster_id_matrix_binning', 'cluster_id_dbscan', 'cluster_id_yousfi'):\n",
    "    print(f\"REPORT FOR METHOD: {method} -----------------\")\n",
    "    print(classification_report(df.is_rec.values, df[method].values.astype(int)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5d68e-65dc-4aad-a7b5-757166cd42fb",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29b35ce2-b073-4b71-ad86-1e5c51b79268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all PK\n",
    "#pks = df.groupby('payment_channel')['is_rec'].sum().sort_values(ascending=False)\n",
    "#pks = pks.index.values.tolist() \n",
    "#print (len(pks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69213540-5435-41f7-8eb6-b5d6c64300f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot some samples\n",
    "#for i in pks[0:5]:\n",
    "#    t = df.loc[(df.payment_channel == i)].copy()\n",
    "#    t[t.amount > 0]\n",
    "#    #t.amount_lvl =  t.amount_lvl.astype(str)\n",
    "#\n",
    "#    print('...............................')\n",
    "#    print(f' primary key: {i}')\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_yousfi'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_matrix_graph_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_matrix_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#    \n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_dbscan'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23e56d-6fcb-4550-b4db-75bf4aa694ac",
   "metadata": {},
   "source": [
    "## Experiment 1: Add new noise to stable series (setting 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6aa62-47c0-4897-af97-ac241c613937",
   "metadata": {},
   "source": [
    "We can repeat the first experiment increasing the number of noise to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91517742-ad5a-4621-9aea-96954f47a403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prob_perturbation = .25  # Probability to add a new point after each point of a serie\n",
    "n_desv_outlier = 3       # multiplier std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8299945f-2b4d-4fd0-a8fc-de18301be334",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436615, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = stable_channels.copy()\n",
    "#df = stable_channels_tmp.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b19c1c26-7079-44cc-b159-3160be8a81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "noised_df = noiser.add_noise_update_exp1(\n",
    "    dataframe=df,\n",
    "    col_pk=var_pk,\n",
    "    col_amnt=var_amnt,\n",
    "    col_date=var_date,\n",
    "    n_desv_outlier=n_desv_outlier,\n",
    "    prob_perturbation=prob_perturbation\n",
    ")\n",
    "\n",
    "noised_df = noised_df.copy().drop(['datediff', 'group_position'], axis=1)\n",
    "print(noised_df.shape)\n",
    "\n",
    "print(\"Computing diffdays...\")\n",
    "# compute diffdays\n",
    "lelele = noised_df.sort_values(['payment_channel', 'date'])\\\n",
    "    .groupby('payment_channel')['date']\\\n",
    "    .diff(1).dt.days\\\n",
    "    .fillna(0)\\\n",
    "    .abs()\n",
    "\n",
    "\n",
    "noised_df = pd.merge(noised_df, lelele.rename('datediff'), left_index=True, right_index=True, how='left')\n",
    "\n",
    "noised_df = noised_df.sort_values(['payment_channel', 'date']).reset_index(drop=True)\n",
    "posid = noised_df.groupby(['payment_channel']).cumcount().rename('group_position')\n",
    "noised_df = noised_df.join(posid)\n",
    "\n",
    "#dup_cols = ['trans_id', 'date', 'amount'] if 'trans_id' in noised_df.columns else ['date', 'amount']\n",
    "#noised_df.drop_duplicates(subset=dup_cols, keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "print (noised_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "143f5e76-ad2d-4327-a31a-6942c6df4afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(noised_df) > len(df), \"Noised DF should be larger since we added points!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714f769-9417-4864-8c6f-5140a9bc71fb",
   "metadata": {},
   "source": [
    "### Assesment Exp 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34becaf0-e142-4dae-b293-0fc72e43765a",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aed3f9e4-fcfe-4771-8178-1a5e7f4b50e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.654989\n",
       "0    0.345011\n",
       "Name: is_rec, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = noised_df.copy()\n",
    "\n",
    "# Save original label\n",
    "df['is_rec_orii'] =  df.is_rec\n",
    "\n",
    "df.is_rec.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6809fa8-ab02-4ba8-b7b5-d258ed992f22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100218/100218 [02:57<00:00, 563.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 6.47 s, total: 3min 5s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = dbscanmethod.main_dbscan_method(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16defcb9-95bc-45fe-9063-b55933d3ca60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cluster_id_dbscan'] = df['is_rec']\n",
    "df['is_rec'] = df['is_rec_orii']\n",
    "#df = df.drop('is_rec_ori', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936661b4-5e05-49ad-9255-439183c0d21a",
   "metadata": {},
   "source": [
    "#### Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1a890e8-9655-47cc-a32e-66df129dfb13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39916/39916 [01:40<00:00, 397.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 1.35 s, total: 1min 38s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def matrix_flag(data, use_dbscan=False):\n",
    "    dates = data[var_date].values\n",
    "    diff_days = data[var_date].diff(1).dt.days.dropna().values\n",
    "    amounts = data[var_amnt].values\n",
    "    orders = data.group_position.values\n",
    "\n",
    "    subseries = matrixmethod.main_matrix_method(diff_days, amounts, use_dbscan=use_dbscan)\n",
    "    \n",
    "    flags = np.ones(len(data)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return pd.Series({'cluster_id_matrix_binning': flags, 'group_position': orders})\n",
    "\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_flag, use_dbscan=False)\\\n",
    "    .explode(['cluster_id_matrix_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])\n",
    "\n",
    "df.loc[df.cluster_id_matrix_binning >= 0, 'cluster_id_matrix_binning'] = 1\n",
    "df.loc[df.cluster_id_matrix_binning < 0, 'cluster_id_matrix_binning'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bc999-4c93-4140-ad48-6e556295867c",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49da5c86-8cd5-47f1-b1dc-d50549c6b8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39916/39916 [06:29<00:00, 102.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67    273213\n",
      "           1       0.82      0.87      0.84    518684\n",
      "\n",
      "    accuracy                           0.79    791897\n",
      "   macro avg       0.77      0.75      0.76    791897\n",
      "weighted avg       0.78      0.79      0.78    791897\n",
      "\n",
      "CPU times: user 6min 16s, sys: 2.92 s, total: 6min 19s\n",
      "Wall time: 6min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def flag_matrix(dates, amounts):    \n",
    "    dates = np.array(dates).astype(np.datetime64)\n",
    "    amounts = np.array(amounts)\n",
    "    \n",
    "    datediffs_to_previous = np.diff(dates).astype('timedelta64[D]')\n",
    "    datediffs_to_previous = datediffs_to_previous/ np.timedelta64(1, 'D')\n",
    "    subseries = graphmethod.main_matrix_method_graphs(datediffs_to_previous, amounts, use_dbscan=False)\n",
    "\n",
    "    flags = np.ones(len(dates)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return flags\n",
    "    \n",
    "def matrix_udf_graphs(data):\n",
    "    dates = data.date.values\n",
    "    amounts = data.amount.values\n",
    "    orders = data.group_position.values\n",
    "    \n",
    "    subseries_ids = flag_matrix(dates, amounts)\n",
    "    \n",
    "    return pd.Series({'cluster_id_matrix_graph_binning': subseries_ids, 'group_position': orders})\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_udf_graphs)\\\n",
    "    .explode(['cluster_id_matrix_graph_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])\n",
    "df.loc[df.cluster_id_matrix_graph_binning >= 0, 'cluster_id_matrix_graph_binning'] = 1\n",
    "df.loc[df.cluster_id_matrix_graph_binning < 0, 'cluster_id_matrix_graph_binning'] = 0\n",
    "\n",
    "print(classification_report(df.is_rec.values, df.cluster_id_matrix_graph_binning.values.astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37d37a-e695-4ee4-814a-33486cbe3526",
   "metadata": {},
   "source": [
    "#### Yousfi (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2ab0c27-b258-4b93-9b67-159417d3f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.type_col='type'\n",
    "        self.client_col= 'payment_channel'\n",
    "        self.customer_id= 'payment_channel'\n",
    "        self.time_col=  'date'\n",
    "        self.amount_col='amount'\n",
    "        self.trans_amount='amount'\n",
    "        self.trans_date=  'date'\n",
    "        self.trans_type=  'type'\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def get_fn(df):\n",
    "    dfs =  list((DetectRecurrencyII(\n",
    "                  trans_data = df,\n",
    "                  client_col= 'payment_channel',\n",
    "                  time_col=  'date',\n",
    "                  amount_col='amount',\n",
    "                  config=config\n",
    "                  )\n",
    "           )[1].values())\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # Add a cluster_id col for all dfs\n",
    "    try:\n",
    "        dfs = [dfs[i].assign(cluster_id = i).reset_index()  for i, d in enumerate(dfs)]\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "\n",
    "    if len(dfs) > 0:\n",
    "        concat_df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "        out = pd.merge(df, \n",
    "              concat_df, \n",
    "             left_on=['date', 'amount'], \n",
    "             right_on=['date', 'amount'], how='left')\n",
    "\n",
    "        dfs = out\n",
    "    else:\n",
    "        dfs = df.assign(cluster_id = -1)\n",
    "\n",
    "    return dfs.loc[:, ['date', 'amount', 'cluster_id']].reset_index(drop=True)    \n",
    "\n",
    "\n",
    "dft = df.copy()\n",
    "\n",
    "outt = dft.sort_values(['payment_channel', 'date']).groupby('payment_channel').apply(lambda x: get_fn(x))\n",
    "dft = pd.merge(dft, outt.reset_index().drop('level_1', axis=1), on=('payment_channel', 'date', 'amount'))\n",
    "dft['cluster_id'] = dft['cluster_id'].fillna(-1).astype(int)\n",
    "\n",
    "df['cluster_id_yousfi'] = dft.cluster_id\n",
    "\n",
    "df.loc[df['cluster_id_yousfi'] >= 0, 'cluster_id_yousfi'] = 1\n",
    "df.loc[df['cluster_id_yousfi'] < 0, 'cluster_id_yousfi'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ec1d9-da05-4797-a72c-cdefa3fcfa79",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32820d3c-bbfa-484d-be43-1e0058d30578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT FOR METHOD: cluster_id_matrix_graph_binning -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67    273213\n",
      "           1       0.82      0.87      0.84    518684\n",
      "\n",
      "    accuracy                           0.79    791897\n",
      "   macro avg       0.77      0.75      0.76    791897\n",
      "weighted avg       0.78      0.79      0.78    791897\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_matrix_binning -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.82    273213\n",
      "           1       0.93      0.85      0.89    518684\n",
      "\n",
      "    accuracy                           0.86    791897\n",
      "   macro avg       0.85      0.87      0.85    791897\n",
      "weighted avg       0.87      0.86      0.87    791897\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_dbscan -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.58      0.61    273213\n",
      "           1       0.79      0.84      0.82    518684\n",
      "\n",
      "    accuracy                           0.75    791897\n",
      "   macro avg       0.72      0.71      0.71    791897\n",
      "weighted avg       0.74      0.75      0.75    791897\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_yousfi -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.85      0.49    273213\n",
      "           1       0.66      0.15      0.24    518684\n",
      "\n",
      "    accuracy                           0.39    791897\n",
      "   macro avg       0.50      0.50      0.37    791897\n",
      "weighted avg       0.55      0.39      0.33    791897\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for method in ('cluster_id_matrix_graph_binning', 'cluster_id_matrix_binning', 'cluster_id_dbscan', 'cluster_id_yousfi'):\n",
    "    print(f\"REPORT FOR METHOD: {method} -----------------\")\n",
    "    print(classification_report(df.is_rec.values, df[method].values.astype(int)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d2168-726a-4d6f-a15c-9fe2b5f39fad",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ce1ec69-cc6d-4cc6-82c1-15575409b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of PK\n",
    "#pks = df.groupby('payment_channel')['is_rec'].sum().sort_values(ascending=False)\n",
    "#pks = pks.index.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bae0ea5-c2a0-4f19-9bf2-6233af7b20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot some samples\n",
    "#for i in pks[0:15]:\n",
    "#    t = df.loc[(df.payment_channel == i)].copy()\n",
    "#    t[t.amount > 0]\n",
    "#    #t.amount_lvl =  t.amount_lvl.astype(str)\n",
    "#\n",
    "#    print('...............................')\n",
    "#    print(f' primary key: {i}')\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_yousfi'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_matrix_graph_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_matrix_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#    \n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_dbscan'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e774478-67ad-4d3a-a8e3-c087a9dec924",
   "metadata": {},
   "source": [
    "## Experiment 2: Merging stable series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8fcf608-e162-4a5a-9461-c6f6a9829ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stable_channels.copy()#.drop('group_position', axis=1)\n",
    "df = df.drop(['group_position','datediff', 'day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "804ead8a-9229-448c-9418-2f393ac936d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "noised_df = noiser.add_noise_update_exp2(\n",
    "                            dataframe=df.copy(),\n",
    "                            col_pk=var_pk,\n",
    "                            col_amnt=var_amnt,\n",
    "                            col_date=var_date,\n",
    "                            prob_combination=1,\n",
    "                            noise_type = 'combine')\n",
    "\n",
    "print(\"Computing diffdays...\")\n",
    "# compute diffdays\n",
    "lelele = noised_df.sort_values(['payment_channel', 'date'])\\\n",
    "    .groupby('payment_channel')['date']\\\n",
    "    .diff(1).dt.days\\\n",
    "    .fillna(0)\\\n",
    "    .abs()\n",
    "\n",
    "#noised_df = noised_df.join(lelele.rename('datediff'))\n",
    "noised_df = pd.merge(noised_df, lelele.rename('datediff'), left_index=True, right_index=True, how='left')\n",
    "#dup_cols = ['trans_id', 'date', 'amount'] if 'trans_id' in noised_df.columns else ['date', 'amount']\n",
    "#noised_df.drop_duplicates(subset=dup_cols, keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "\n",
    "noised_df = noised_df.sort_values(['payment_channel', 'date']).reset_index(drop=True)\n",
    "posid = noised_df.groupby(['payment_channel']).cumcount().rename('group_position')\n",
    "noised_df = noised_df.drop('group_position', axis=1).join(posid)\n",
    "\n",
    "n_orig = noised_df.groupby('payment_channel')['payment_channel_ori'].nunique().sort_values()\n",
    "noised_df = noised_df.loc[noised_df.payment_channel.isin(set(n_orig[n_orig > 1].index.tolist()))]\n",
    "\n",
    "\n",
    "# Execute only when combining series\n",
    "assert np.isclose(df.payment_channel.nunique() / 2, noised_df.payment_channel.nunique(), atol=2), \"This should be half the first\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06169ef3-eb93-4c6d-9930-28443f014830",
   "metadata": {},
   "source": [
    "### Assesment Exp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2262164d-e1a2-41ab-b033-7e4f08a791db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1.0\n",
       "Name: is_rec, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = noised_df.copy()\n",
    "\n",
    "# Save original label\n",
    "df['is_rec_orii'] =  df.is_rec\n",
    "\n",
    "df.is_rec.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7bdb2-68b7-42bb-abb2-2e6cabb69f1a",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3aad63a0-a189-492a-8e5f-93f54beb0de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['is_rec_ori'] = df['is_rec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed54a920-2c07-4455-9341-27563432195b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38802/38802 [01:33<00:00, 416.91it/s]\n"
     ]
    }
   ],
   "source": [
    "df = dbscanmethod.main_dbscan_method(df)\n",
    "\n",
    "df['cluster_id_dbscan'] = df['amount_lvl']*10 + df['day_cluster']  \n",
    "df['cluster_id_dbscan'] = df.apply(lambda x: -1 if x['is_rec']==0 else x['cluster_id_dbscan'], axis=1)\n",
    "#df = df.drop(['day_cluster','amount_lvl'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98784bc-503c-434b-9e3a-606d3b88ea89",
   "metadata": {},
   "source": [
    "#### Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00225560-9c3f-4f92-a446-bf5b3cb49a38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19958/19958 [00:50<00:00, 395.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 s, sys: 605 ms, total: 50.3 s\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def matrix_flag(data, use_dbscan=False):\n",
    "    dates = data[var_date].values\n",
    "    diff_days = data[var_date].diff(1).dt.days.dropna().values\n",
    "    amounts = data[var_amnt].values\n",
    "    orders = data.group_position.values\n",
    "\n",
    "    subseries = matrixmethod.main_matrix_method(diff_days, amounts, use_dbscan=use_dbscan)\n",
    "    \n",
    "    flags = np.ones(len(data)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return pd.Series({'cluster_id_matrix_binning': flags, 'group_position': orders})\n",
    "\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_flag, use_dbscan=False)\\\n",
    "    .explode(['cluster_id_matrix_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec5627-de9f-462f-9dc9-6e9f6630ee70",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd29e80b-bc7f-428b-820c-2dbeca0e7192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19958/19958 [03:31<00:00, 94.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 27s, sys: 1.23 s, total: 3min 28s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def flag_matrix(dates, amounts):    \n",
    "    dates = np.array(dates).astype(np.datetime64)\n",
    "    amounts = np.array(amounts)\n",
    "\n",
    "    datediffs_to_previous = np.diff(dates).astype('timedelta64[D]')\n",
    "    datediffs_to_previous = datediffs_to_previous/ np.timedelta64(1, 'D')\n",
    "\n",
    "    subseries = graphmethod.main_matrix_method_graphs(datediffs_to_previous, amounts, use_dbscan=False)\n",
    "\n",
    "    flags = np.ones(len(dates)) * -1\n",
    "    \n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return flags\n",
    "    \n",
    "def matrix_udf_graphs(data):\n",
    "    dates = data.date.values\n",
    "    amounts = data.amount.values\n",
    "    orders = data.group_position.values\n",
    "    \n",
    "    subseries_ids = flag_matrix(dates, amounts)\n",
    "    \n",
    "    return pd.Series({'cluster_id_matrix_graph_binning': subseries_ids, 'group_position': orders})\n",
    "    \n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_udf_graphs)\\\n",
    "    .explode(['cluster_id_matrix_graph_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a516e-c290-444f-823f-8e201692519a",
   "metadata": {},
   "source": [
    "#### Yousfi (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e82e218d-4add-4269-9f09-a8fc3d97dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# YOUSFI  ######################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.type_col='type'\n",
    "        self.client_col= 'payment_channel'\n",
    "        self.customer_id= 'payment_channel'\n",
    "        self.time_col=  'date'\n",
    "        self.amount_col='amount'\n",
    "        self.trans_amount='amount'\n",
    "        self.trans_date=  'date'\n",
    "        self.trans_type=  'type'\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def get_fn(df):\n",
    "    dfs =  list((DetectRecurrencyII(\n",
    "                  trans_data = df,\n",
    "                  client_col= 'payment_channel',\n",
    "                  time_col=  'date',\n",
    "                  amount_col='amount',\n",
    "                  config=config\n",
    "                  )\n",
    "           )[1].values())\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # Add a cluster_id col for all dfs\n",
    "    try:\n",
    "        dfs = [dfs[i].assign(cluster_id = i).reset_index()  for i, d in enumerate(dfs)]\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "\n",
    "    if len(dfs) > 0:\n",
    "        concat_df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "        out = pd.merge(df, \n",
    "              concat_df, \n",
    "             left_on=['date', 'amount'], \n",
    "             right_on=['date', 'amount'], how='left')\n",
    "\n",
    "        dfs = out\n",
    "    else:\n",
    "        dfs = df.assign(cluster_id = -1)\n",
    "\n",
    "    return dfs.loc[:, ['date', 'amount', 'cluster_id']].reset_index(drop=True)    \n",
    "\n",
    "\n",
    "\n",
    "dft = df.copy()\n",
    "\n",
    "outt = dft.sort_values(['payment_channel', 'date']).groupby('payment_channel').apply(lambda x: get_fn(x))\n",
    "dft = pd.merge(dft, outt.reset_index().drop('level_1', axis=1), on=('payment_channel', 'date', 'amount'))\n",
    "dft['cluster_id'] = dft['cluster_id'].fillna(-1).astype(int)\n",
    "\n",
    "df['cluster_id_yousfi'] = dft.cluster_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b901d-392e-4c49-a6b9-bcf8d1976e0b",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f31131b-fbd8-4155-a2df-cc8a28c8400c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19958, 2) (19958, 2) (19956, 2) (19958, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compute the total number of series for each algorithm, for each PK (i.e. payment channel)\n",
    "df_agg = pd.DataFrame(df.groupby([var_pk], as_index=False)[var_date].nunique( ))\n",
    "df_agg.columns=[var_pk,'counts']\n",
    "matrix_graph_binning = pd.DataFrame(df[df['cluster_id_matrix_graph_binning']>-1].groupby([var_pk],as_index=False)['cluster_id_matrix_graph_binning'].nunique())\n",
    "matrix_graph_binning.columns=[var_pk,'matrix_graph_binning']\n",
    "matrix_binning = pd.DataFrame(df[df['cluster_id_matrix_binning']>-1].groupby([var_pk],as_index=False)['cluster_id_matrix_binning'].nunique())\n",
    "matrix_binning.columns=[var_pk,'matrix_binning']\n",
    "dbscan = pd.DataFrame(df[df['cluster_id_dbscan']>-1].groupby([var_pk],as_index=False)['cluster_id_dbscan'].nunique())\n",
    "dbscan.columns=[var_pk,'dbscan']\n",
    "yousfi = pd.DataFrame(df[df['cluster_id_yousfi']>-1].groupby([var_pk],as_index=False)['cluster_id_yousfi'].nunique())\n",
    "yousfi.columns=[var_pk,'yousfi']\n",
    "\n",
    "print(df_agg.shape,matrix_graph_binning.shape,matrix_binning.shape,dbscan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c08ee6e8-369e-4d81-bb5a-cbb0c9df2841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Exact splitting to two series\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "match_matrix_graph_binning    0.907556\n",
       "match_matrix_binning          0.705031\n",
       "match_dbscan                  0.942128\n",
       "match_yousfi                  0.042088\n",
       "Name: mean, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the exact match of 2 identified clusters (the closer to 1, the better)\n",
    "print(\"Percent Exact splitting to two series\")\n",
    "df_all = df_agg.merge(matrix_graph_binning, on=[var_pk], how='left')\n",
    "df_all = df_all.merge(matrix_binning, on=[var_pk], how='left')\n",
    "df_all = df_all.merge(dbscan, on=[var_pk], how='left')\n",
    "df_all = df_all.merge(yousfi, on=[var_pk], how='left')\n",
    "\n",
    "df_all['match_matrix_graph_binning'] = df_all['matrix_graph_binning'].apply(lambda x: 1 if x==2 else 0)\n",
    "df_all['match_matrix_binning'] = df_all['matrix_binning'].apply(lambda x: 1 if x==2 else 0)\n",
    "df_all['match_dbscan'] = df_all['dbscan'].apply(lambda x: 1 if x==2 else 0)\n",
    "df_all['match_yousfi'] = df_all['yousfi'].apply(lambda x: 1 if x==2 else 0)\n",
    "df_all[['match_matrix_graph_binning',\n",
    "       'match_matrix_binning', 'match_dbscan', 'match_yousfi']].describe().loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a0ab86b-fec5-4843-b9f3-f550ee5db690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Matching points to original subseries for method dbscan = 0.9744693202917146\n",
      "Avg Matching points to original subseries for method matrix_binning = 0.872627052732988\n",
      "Avg Matching points to original subseries for method matrix_graph_binning = 0.9545087164992907\n",
      "Avg Matching points to original subseries for method yousfi = 0.584198142122758\n"
     ]
    }
   ],
   "source": [
    "# For each of the original subseries, we set the \"predicted cluster\" as the maximum number of predictions on that subserie. \n",
    "# I.e. If a subseries gets the most of its points predicted as cluster_id = 2, then the subseries will be considered cluster = 2 and \n",
    "# the overlap amount will be measured with that cluster in mind (number of preds = 2 / total number of points in that suberie). \n",
    "# The same will be done with the other subserie.\n",
    "# It can be the case in which both subseries are predicted within the same cluster_id. In that case, the overlap metric will be: \n",
    "#      number_of_ones in the shorter subserie / number of points on the entire (merged) serie.\n",
    "# the final metric is the weighted average between the above two cases.\n",
    "\n",
    "cluster_modes = df.groupby('payment_channel_ori').agg({'cluster_id_dbscan': lambda x: x.value_counts().index[0], 'cluster_id_matrix_binning': lambda x: x.value_counts().index[0], 'cluster_id_matrix_graph_binning': lambda x: x.value_counts().index[0], 'cluster_id_yousfi': lambda x: x.value_counts().index[0] })\n",
    "cluster_modes.columns = ['mode_cluster_id_dbscan', 'mode_cluster_id_matrix_binning', 'mode_cluster_id_matrix_graph_binning', 'mode_cluster_id_yousfi']\n",
    "\n",
    "dff = df.copy()\n",
    "\n",
    "dff = dff.join(cluster_modes, on='payment_channel_ori')\n",
    "\n",
    "dff = dff.join(\n",
    "    dff.groupby('payment_channel_ori')['payment_channel_ori'].count().rename('series_len'), on='payment_channel_ori'\n",
    ")\n",
    "\n",
    "for m in ('dbscan', 'matrix_binning', 'matrix_graph_binning', 'yousfi'):\n",
    "    uniques = dff.groupby('payment_channel')[f'cluster_id_{m}'].nunique()\n",
    "    uniques = uniques[uniques < 2]\n",
    "\n",
    "    metric_oks = (dff.loc[~dff.payment_channel.isin(set(uniques.index.tolist())), f'mode_cluster_id_{m}']  == dff.loc[~dff.payment_channel.isin(set(uniques.index.tolist())), f'cluster_id_{m}']).mean()\n",
    "    metric_no_oks = (dff.loc[dff.payment_channel.isin(set(uniques.index.tolist()))].groupby('payment_channel')['series_len'].min() / dff.loc[dff.payment_channel.isin(set(uniques.index.tolist()))].groupby('payment_channel')['payment_channel'].count()).mean()\n",
    "    if np.isnan(metric_no_oks):\n",
    "        metric_no_oks = 0\n",
    "    \n",
    "    #weighted average\n",
    "    proportion_no_oks = len(uniques) / dff.payment_channel.nunique()\n",
    "    avg = metric_oks * (1 - proportion_no_oks) + metric_no_oks * proportion_no_oks\n",
    "    print(f\"Avg Matching points to original subseries for method {m} = {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99ac1b4a-4764-4eae-be67-34abd19188ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Splitting of the series (2 is the optimum value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cluster_id_matrix_graph_binning    2.045546\n",
       "cluster_id_matrix_binning          2.131727\n",
       "cluster_id_dbscan                  1.968584\n",
       "cluster_id_yousfi                  0.438922\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Avg. Splitting of the series (2 is the optimum value)')\n",
    "\n",
    "unique_ignoring = lambda x: len(np.unique(x[x>=0]))\n",
    "unique_ignoring.__name__ = 'unique_ignoring'\n",
    "\n",
    "df.groupby(var_pk).agg({'cluster_id_matrix_graph_binning': unique_ignoring, \n",
    "                                  'cluster_id_matrix_binning': unique_ignoring,\n",
    "                                  'cluster_id_dbscan': unique_ignoring,\n",
    "                                  'cluster_id_yousfi': unique_ignoring\n",
    "                       }).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b24bfc-b333-4dbb-b1ea-3180dc4034a5",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb750f7f-d89f-4603-88c0-b29b06cfbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of PK\n",
    "#pks = df.groupby('payment_channel')['is_rec'].sum().sort_values(ascending=False)\n",
    "#pks = pks.index.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "322b63a3-436b-4f00-a224-cf9a13a54aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['day'] = df['date'].apply(lambda r:r.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fcbed62-75b2-448e-9107-117e873d3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot some samples\n",
    "#for i in pks[0:15]:\n",
    "#    t = df.loc[(df.payment_channel == i)].copy()\n",
    "#    t[t.amount > 0]\n",
    "#    #t.amount_lvl =  t.amount_lvl.astype(str)\n",
    "#\n",
    "#    print('...............................')\n",
    "#    print(f' primary key: {i}')\n",
    "#    days = t['day'].values.tolist() \n",
    "#    diffdays = t['date_diffdays'].values.tolist()\n",
    "#\n",
    "#    print(f'day of the month array: {days}')\n",
    "#    print(f'diff days: {diffdays}')\n",
    "#    \n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='payment_channel_ori',\n",
    "#                    hue='cluster_id_yousfi'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='payment_channel_ori',\n",
    "#                    hue='cluster_id_matrix_graph_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='payment_channel_ori',\n",
    "#                    hue='cluster_id_matrix_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#    \n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='payment_channel_ori',\n",
    "#                    hue='cluster_id_dbscan'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59a81e-e02f-49e5-ae81-d5266e756632",
   "metadata": {},
   "source": [
    "## Experiment 3.1: Perturb points in the original stable series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d7c26-74b1-4091-a0a6-b69575172ed3",
   "metadata": {},
   "source": [
    "Only move the already existing points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa74eb58-83c9-4788-a1f0-c5e53f00c759",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436615, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = stable_channels.copy()#.drop('group_position', axis=1)\n",
    "df = df.drop(['group_position','datediff', 'day'], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbfb517b-6745-4d48-9903-bf7d0dec8d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 412 ms, total: 18.3 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "noised_df = noiser.add_noise_update_exp3(\n",
    "                            dataframe=df.copy(),\n",
    "                            col_pk=var_pk,\n",
    "                            col_amnt=var_amnt,\n",
    "                            col_date=var_date,\n",
    "                            run_noise_1=False)  # Do not add new noise\n",
    "\n",
    "noised_df = noised_df.copy()#.drop(['datediff', 'group_position'], axis=1)\n",
    "\n",
    "print(\"Computing diffdays...\")\n",
    "# compute diffdays\n",
    "lelele = noised_df.sort_values(['payment_channel', 'date'])\\\n",
    "    .groupby('payment_channel')['date']\\\n",
    "    .diff(1).dt.days\\\n",
    "    .fillna(0)\\\n",
    "    .abs()\n",
    "\n",
    "#noised_df = noised_df.join(lelele.rename('datediff'))\n",
    "noised_df = pd.merge(noised_df, lelele.rename('datediff'), left_index=True, right_index=True, how='left')\n",
    "dup_cols = ['trans_id', 'date', 'amount'] if 'trans_id' in noised_df.columns else ['date', 'amount']\n",
    "noised_df.drop_duplicates(subset=dup_cols, keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "\n",
    "noised_df = noised_df.sort_values(['payment_channel', 'date']).reset_index(drop=True)\n",
    "posid = noised_df.groupby(['payment_channel']).cumcount().rename('group_position')\n",
    "noised_df = noised_df.join(posid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51a0f6a0-709b-4574-9b44-4fefccfba5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.830246\n",
       "0    0.169754\n",
       "Name: is_rec, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_df.is_rec.value_counts() / len(noised_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba55df-f3b6-4f04-80f7-9a3bce8b74b0",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b582d982-9a3d-499d-b95c-2027e034e7dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.830246\n",
       "0    0.169754\n",
       "Name: is_rec, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = noised_df.copy()\n",
    "\n",
    "# Save original label\n",
    "df['is_rec_orii'] =  df.is_rec\n",
    "\n",
    "df.is_rec.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e85b424-430f-4f1a-88e5-7f215b0e2c36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32008/32008 [00:32<00:00, 975.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.7 s, sys: 1.38 s, total: 38 s\n",
      "Wall time: 37.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = dbscanmethod.main_dbscan_method(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8a9c65b-19d4-49f9-93ba-14aa2e3cf7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cluster_id_dbscan'] = df['is_rec']\n",
    "df['is_rec'] = df['is_rec_orii']\n",
    "#df = df.drop('is_rec_ori', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4139b-bc3c-4192-b18c-0a7f325529fa",
   "metadata": {},
   "source": [
    "#### Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33229a44-3830-4a04-8ed2-057133de1a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27521/27521 [00:18<00:00, 1518.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 233 ms, total: 18 s\n",
      "Wall time: 18.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def matrix_flag(data, use_dbscan=False):\n",
    "    dates = data[var_date].values\n",
    "    diff_days = data[var_date].diff(1).dt.days.dropna().values\n",
    "    amounts = data[var_amnt].values\n",
    "    orders = data.group_position.values\n",
    "\n",
    "    subseries = matrixmethod.main_matrix_method(diff_days, amounts, use_dbscan=use_dbscan)\n",
    "    \n",
    "    flags = np.ones(len(data)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return pd.Series({'cluster_id_matrix_binning': flags, 'group_position': orders})\n",
    "\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_flag, use_dbscan=False)\\\n",
    "    .explode(['cluster_id_matrix_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])\n",
    "\n",
    "df.loc[df.cluster_id_matrix_binning >= 0, 'cluster_id_matrix_binning'] = 1\n",
    "df.loc[df.cluster_id_matrix_binning < 0, 'cluster_id_matrix_binning'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cbc5d0-e9a7-4d15-997a-ec1ab5a88713",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b4fe12c-bd66-40e0-8501-34c104138834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27521/27521 [00:15<00:00, 1756.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.66     21876\n",
      "           1       0.99      0.81      0.89    106993\n",
      "\n",
      "    accuracy                           0.83    128869\n",
      "   macro avg       0.75      0.89      0.78    128869\n",
      "weighted avg       0.91      0.83      0.85    128869\n",
      "\n",
      "CPU times: user 15.6 s, sys: 240 ms, total: 15.8 s\n",
      "Wall time: 15.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def flag_matrix(dates, amounts):    \n",
    "    dates = np.array(dates).astype(np.datetime64)\n",
    "    amounts = np.array(amounts)\n",
    "    \n",
    "    datediffs_to_previous = np.diff(dates).astype('timedelta64[D]')\n",
    "    datediffs_to_previous = datediffs_to_previous/ np.timedelta64(1, 'D')\n",
    "    subseries = graphmethod.main_matrix_method_graphs(datediffs_to_previous, amounts, use_dbscan=False)\n",
    "\n",
    "    flags = np.ones(len(dates)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return flags\n",
    "    \n",
    "def matrix_udf_graphs(data):\n",
    "    dates = data.date.values\n",
    "    amounts = data.amount.values\n",
    "    orders = data.group_position.values\n",
    "    \n",
    "    subseries_ids = flag_matrix(dates, amounts)\n",
    "    \n",
    "    return pd.Series({'cluster_id_matrix_graph_binning': subseries_ids, 'group_position': orders})\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_udf_graphs)\\\n",
    "    .explode(['cluster_id_matrix_graph_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])\n",
    "df.loc[df.cluster_id_matrix_graph_binning >= 0, 'cluster_id_matrix_graph_binning'] = 1\n",
    "df.loc[df.cluster_id_matrix_graph_binning < 0, 'cluster_id_matrix_graph_binning'] = 0\n",
    "\n",
    "print(classification_report(df.is_rec.values, df.cluster_id_matrix_graph_binning.values.astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcda005-5243-4469-ac22-cec9a5efcc29",
   "metadata": {},
   "source": [
    "#### Yousfi (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89511d30-61fe-499c-a024-9d4a410f550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.type_col='type'\n",
    "        self.client_col= 'payment_channel'\n",
    "        self.customer_id= 'payment_channel'\n",
    "        self.time_col=  'date'\n",
    "        self.amount_col='amount'\n",
    "        self.trans_amount='amount'\n",
    "        self.trans_date=  'date'\n",
    "        self.trans_type=  'type'\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def get_fn(df):\n",
    "    dfs =  list((DetectRecurrencyII(\n",
    "                  trans_data = df,\n",
    "                  client_col= 'payment_channel',\n",
    "                  time_col=  'date',\n",
    "                  amount_col='amount',\n",
    "                  config=config\n",
    "                  )\n",
    "           )[1].values())\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # Add a cluster_id col for all dfs\n",
    "    try:\n",
    "        dfs = [dfs[i].assign(cluster_id = i).reset_index()  for i, d in enumerate(dfs)]\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "\n",
    "    if len(dfs) > 0:\n",
    "        concat_df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "        out = pd.merge(df, \n",
    "              concat_df, \n",
    "             left_on=['date', 'amount'], \n",
    "             right_on=['date', 'amount'], how='left')\n",
    "\n",
    "        dfs = out\n",
    "    else:\n",
    "        dfs = df.assign(cluster_id = -1)\n",
    "\n",
    "    return dfs.loc[:, ['date', 'amount', 'cluster_id']].reset_index(drop=True)    \n",
    "\n",
    "\n",
    "\n",
    "dft = df.copy()\n",
    "\n",
    "outt = dft.sort_values(['payment_channel', 'date']).groupby('payment_channel').apply(lambda x: get_fn(x))\n",
    "dft = pd.merge(dft, outt.reset_index().drop('level_1', axis=1), on=('payment_channel', 'date', 'amount'))\n",
    "dft['cluster_id'] = dft['cluster_id'].fillna(-1).astype(int)\n",
    "\n",
    "df['cluster_id_yousfi'] = dft.cluster_id\n",
    "\n",
    "df.loc[df['cluster_id_yousfi'] >= 0, 'cluster_id_yousfi'] = 1\n",
    "df.loc[df['cluster_id_yousfi'] < 0, 'cluster_id_yousfi'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42db7b-6f73-4811-a14b-eea613d83f4d",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "173355bf-a3f7-406d-a111-56b768468586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT FOR METHOD: cluster_id_matrix_graph_binning -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.66     21876\n",
      "           1       0.99      0.81      0.89    106993\n",
      "\n",
      "    accuracy                           0.83    128869\n",
      "   macro avg       0.75      0.89      0.78    128869\n",
      "weighted avg       0.91      0.83      0.85    128869\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_matrix_binning -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.96      0.54     21876\n",
      "           1       0.99      0.68      0.81    106993\n",
      "\n",
      "    accuracy                           0.73    128869\n",
      "   macro avg       0.68      0.82      0.67    128869\n",
      "weighted avg       0.88      0.73      0.76    128869\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_dbscan -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77     21876\n",
      "           1       1.00      0.88      0.94    106993\n",
      "\n",
      "    accuracy                           0.90    128869\n",
      "   macro avg       0.82      0.94      0.86    128869\n",
      "weighted avg       0.94      0.90      0.91    128869\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_yousfi -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.85      0.28     21876\n",
      "           1       0.83      0.16      0.26    106993\n",
      "\n",
      "    accuracy                           0.27    128869\n",
      "   macro avg       0.50      0.50      0.27    128869\n",
      "weighted avg       0.72      0.27      0.27    128869\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for method in ('cluster_id_matrix_graph_binning', 'cluster_id_matrix_binning', 'cluster_id_dbscan', 'cluster_id_yousfi'):\n",
    "    print(f\"REPORT FOR METHOD: {method} -----------------\")\n",
    "    print(classification_report(df.is_rec.values, df[method].values.astype(int)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac34c27-ab16-4764-b9a1-9eabeb59dea7",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8d7d15b-b829-4b04-b4ca-54611127f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of PK\n",
    "#pks = df.groupby('payment_channel')['is_rec'].sum().sort_values(ascending=False)\n",
    "#pks = pks.index.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb6e06e6-b3b7-4f4a-bad6-e278349f40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['day'] = df['date'].apply(lambda r:r.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d9200fb-3b4b-4b0b-8a0d-48057075c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot some samples\n",
    "#for i in pks[0:15]:\n",
    "#    t = df.loc[(df.payment_channel == i)].copy()\n",
    "#    t[t.amount > 0]\n",
    "#    #t.amount_lvl =  t.amount_lvl.astype(str)\n",
    "#\n",
    "#\n",
    "#    print('...............................')\n",
    "#    print(f' primary key: {i}')\n",
    "#    days = t['day'].values.tolist() \n",
    "#    diffdays = t['date_diffdays'].values.tolist()\n",
    "#\n",
    "#    print(f'day of the month array: {days}')\n",
    "#    print(f'diff days: {diffdays}')\n",
    "#    \n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_yousfi'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"upper left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_matrix_graph_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"upper left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_matrix_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"upper left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#    \n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_dbscan'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"upper left\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08801bf-127f-4a1e-8cd6-7c6a1eb64782",
   "metadata": {},
   "source": [
    "## Experiment 3.2: Perturb points in the original stable series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac665e-410a-4801-9af1-c2fd1604ffc7",
   "metadata": {},
   "source": [
    "Move the already existing points AND add new noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "656694a7-7322-4a24-bee3-6fe2a19b1b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = stable_channels.copy().drop('group_position', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21c22ccc-7d33-47eb-afbf-a10912ae8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(stable_channels[stable_channels['payment_channel']=='7445-21391058-PREVOD NA UCET-VYDAJ'].shape)\n",
    "#df1 = df[['payment_channel','date','amount']]\n",
    "#df1[df1['payment_channel']=='7445-21391058-PREVOD NA UCET-VYDAJ'].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b89504b8-f53b-4050-861a-1d9c25421174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 s, sys: 614 ms, total: 26.7 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "noised_df = noiser.add_noise_update_exp3(\n",
    "                            dataframe=df.copy(),\n",
    "                            col_pk=var_pk,\n",
    "                            col_amnt=var_amnt,\n",
    "                            col_date=var_date,\n",
    "                            run_noise_1=True)  # add new noise\n",
    "\n",
    "noised_df = noised_df.copy()#.drop(['datediff', 'group_position'], axis=1)\n",
    "\n",
    "print(\"Computing diffdays...\")\n",
    "# compute diffdays\n",
    "lelele = noised_df.sort_values(['payment_channel', 'date'])\\\n",
    "    .groupby('payment_channel')['date']\\\n",
    "    .diff(1).dt.days\\\n",
    "    .fillna(0)\\\n",
    "    .abs()\n",
    "\n",
    "#noised_df = noised_df.join(lelele.rename('datediff'))\n",
    "noised_df = pd.merge(noised_df, lelele.rename('datediff'), left_index=True, right_index=True, how='left')\n",
    "#dup_cols = ['trans_id', 'date', 'amount'] if 'trans_id' in noised_df.columns else ['date', 'amount']\n",
    "#noised_df.drop_duplicates(subset=dup_cols, keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "noised_df = noised_df.sort_values(['payment_channel', 'date']).reset_index(drop=True)\n",
    "posid = noised_df.groupby(['payment_channel']).cumcount().rename('group_position')\n",
    "noised_df = noised_df.join(posid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c46011c4-5621-43e7-acd3-f6c0ea17e7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.839926\n",
       "0    0.160074\n",
       "Name: is_rec, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_df.is_rec.value_counts() / len(noised_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb90c72-a746-42d0-b6a0-256e2f65ec92",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62cdb66e-48ec-43f1-8a92-1e9496636108",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.839926\n",
       "0    0.160074\n",
       "Name: is_rec, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = noised_df.copy()\n",
    "\n",
    "\n",
    "# Save original label\n",
    "df['is_rec_orii'] =  df.is_rec\n",
    "\n",
    "df.is_rec.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f94c0f17-e5f1-454d-9e81-8d804737ee18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74140/74140 [02:01<00:00, 607.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 3s, sys: 4.76 s, total: 2min 8s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = dbscanmethod.main_dbscan_method(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58e981cf-ae9a-4c7b-b185-3d37cc3d65f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cluster_id_dbscan'] = df['is_rec']\n",
    "df['is_rec'] = df['is_rec_orii']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465817df-2e11-4d42-b89e-0981e3cef6d6",
   "metadata": {},
   "source": [
    "#### Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcc5f7a1-e3d2-4f26-b6b2-f1d0f5349076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39916/39916 [00:53<00:00, 744.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 s, sys: 507 ms, total: 52.5 s\n",
      "Wall time: 54.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def matrix_flag(data, use_dbscan=False):\n",
    "    dates = data[var_date].values\n",
    "    diff_days = data[var_date].diff(1).dt.days.dropna().values\n",
    "    amounts = data[var_amnt].values\n",
    "    orders = data.group_position.values\n",
    "\n",
    "    subseries = matrixmethod.main_matrix_method(diff_days, amounts, use_dbscan=use_dbscan)\n",
    "    \n",
    "    flags = np.ones(len(data)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return pd.Series({'cluster_id_matrix_binning': flags, 'group_position': orders})\n",
    "\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_flag, use_dbscan=False)\\\n",
    "    .explode(['cluster_id_matrix_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])\n",
    "\n",
    "df.loc[df.cluster_id_matrix_binning >= 0, 'cluster_id_matrix_binning'] = 1\n",
    "df.loc[df.cluster_id_matrix_binning < 0, 'cluster_id_matrix_binning'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb632c3-c331-40e1-818b-6a67dd2325a6",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9cd0f1e-c305-44ec-ab7b-0b88e576f310",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39916/39916 [01:37<00:00, 411.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 914 ms, total: 1min 36s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def flag_matrix(dates, amounts):    \n",
    "    dates = np.array(dates).astype(np.datetime64)\n",
    "    amounts = np.array(amounts)\n",
    "    \n",
    "    datediffs_to_previous = np.diff(dates).astype('timedelta64[D]')\n",
    "    datediffs_to_previous = datediffs_to_previous/ np.timedelta64(1, 'D')\n",
    "    subseries = graphmethod.main_matrix_method_graphs(datediffs_to_previous, amounts, use_dbscan=False)\n",
    "\n",
    "    flags = np.ones(len(dates)) * -1\n",
    "\n",
    "    # Return array with subseries ids (eg: [0,1,1,1,1,1,0,0,2,0,2,2,2,2])\n",
    "    l = list(enumerate(subseries))\n",
    "    for i, indices in l:\n",
    "        flags[indices] = i\n",
    "\n",
    "    return flags\n",
    "    \n",
    "def matrix_udf_graphs(data):\n",
    "    dates = data.date.values\n",
    "    amounts = data.amount.values\n",
    "    orders = data.group_position.values\n",
    "    \n",
    "    subseries_ids = flag_matrix(dates, amounts)\n",
    "    \n",
    "    return pd.Series({'cluster_id_matrix_graph_binning': subseries_ids, 'group_position': orders})\n",
    "\n",
    "bin_nbs = df.sort_values([var_pk, var_date])\\\n",
    "    .groupby([var_pk])\\\n",
    "    .progress_apply(matrix_udf_graphs)\\\n",
    "    .explode(['cluster_id_matrix_graph_binning', 'group_position'])\n",
    "\n",
    "df = pd.merge(df, bin_nbs, on=[var_pk, 'group_position'])\n",
    "df.loc[df.cluster_id_matrix_graph_binning >= 0, 'cluster_id_matrix_graph_binning'] = 1\n",
    "df.loc[df.cluster_id_matrix_graph_binning < 0, 'cluster_id_matrix_graph_binning'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320cd2a-4dda-4cfa-81a4-9b0a6f4b0f04",
   "metadata": {},
   "source": [
    "#### Yousfi (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a1d1f216-212e-4bad-9508-4a7befe2d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.type_col='type'\n",
    "        self.client_col= 'payment_channel'\n",
    "        self.customer_id= 'payment_channel'\n",
    "        self.time_col=  'date'\n",
    "        self.amount_col='amount'\n",
    "        self.trans_amount='amount'\n",
    "        self.trans_date=  'date'\n",
    "        self.trans_type=  'type'\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def get_fn(df):\n",
    "    dfs =  list((DetectRecurrencyII(\n",
    "                  trans_data = df,\n",
    "                  client_col= 'payment_channel',\n",
    "                  time_col=  'date',\n",
    "                  amount_col='amount',\n",
    "                  config=config\n",
    "                  )\n",
    "           )[1].values())\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # Add a cluster_id col for all dfs\n",
    "    try:\n",
    "        dfs = [dfs[i].assign(cluster_id = i).reset_index()  for i, d in enumerate(dfs)]\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "\n",
    "    if len(dfs) > 0:\n",
    "        concat_df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "        out = pd.merge(df, \n",
    "              concat_df, \n",
    "             left_on=['date', 'amount'], \n",
    "             right_on=['date', 'amount'], how='left')\n",
    "\n",
    "        dfs = out\n",
    "    else:\n",
    "        dfs = df.assign(cluster_id = -1)\n",
    "\n",
    "    return dfs.loc[:, ['date', 'amount', 'cluster_id']].reset_index(drop=True)    \n",
    "\n",
    "\n",
    "\n",
    "dft = df.copy()\n",
    "\n",
    "outt = dft.sort_values(['payment_channel', 'date']).groupby('payment_channel').apply(lambda x: get_fn(x))\n",
    "dft = pd.merge(dft, outt.reset_index().drop('level_1', axis=1), on=('payment_channel', 'date', 'amount'))\n",
    "dft['cluster_id'] = dft['cluster_id'].fillna(-1).astype(int)\n",
    "\n",
    "df['cluster_id_yousfi'] = dft.cluster_id\n",
    "\n",
    "df.loc[df['cluster_id_yousfi'] >= 0, 'cluster_id_yousfi'] = 1\n",
    "df.loc[df['cluster_id_yousfi'] < 0, 'cluster_id_yousfi'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e04ba6-bc58-4fa6-9a43-b11d6ac6c260",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1dca504b-937b-4a12-9dde-c6393673dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT FOR METHOD: cluster_id_matrix_graph_binning -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.91      0.75     83372\n",
      "           1       0.98      0.90      0.94    437463\n",
      "\n",
      "    accuracy                           0.91    520835\n",
      "   macro avg       0.81      0.91      0.85    520835\n",
      "weighted avg       0.93      0.91      0.91    520835\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_matrix_binning -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.93      0.62     83372\n",
      "           1       0.98      0.80      0.88    437463\n",
      "\n",
      "    accuracy                           0.82    520835\n",
      "   macro avg       0.73      0.86      0.75    520835\n",
      "weighted avg       0.90      0.82      0.84    520835\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_dbscan -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.77     83372\n",
      "           1       0.97      0.93      0.95    437463\n",
      "\n",
      "    accuracy                           0.92    520835\n",
      "   macro avg       0.84      0.89      0.86    520835\n",
      "weighted avg       0.93      0.92      0.92    520835\n",
      "\n",
      "\n",
      "REPORT FOR METHOD: cluster_id_yousfi -----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.90      0.27     83372\n",
      "           1       0.84      0.10      0.19    437463\n",
      "\n",
      "    accuracy                           0.23    520835\n",
      "   macro avg       0.50      0.50      0.23    520835\n",
      "weighted avg       0.73      0.23      0.20    520835\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for method in ('cluster_id_matrix_graph_binning', 'cluster_id_matrix_binning', 'cluster_id_dbscan', 'cluster_id_yousfi'):\n",
    "    print(f\"REPORT FOR METHOD: {method} -----------------\")\n",
    "    print(classification_report(df.is_rec.values, df[method].values.astype(int)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6b86a-82c1-4569-b276-f8480a0575b2",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ccc30232-2038-4c06-bfab-21bd5a96ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of PK\n",
    "#pks = df.groupby('payment_channel')['is_rec'].sum().sort_values(ascending=False)\n",
    "#pks = pks.index.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1ab73efe-5bf0-44a8-b7f1-f2b567b562ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot some samples\n",
    "#for i in ['7445-21391058-PREVOD NA UCET-VYDAJ']+pks[0:15]:\n",
    "#    t = df.loc[(df.payment_channel == i)].copy()\n",
    "#    t[t.amount > 0]\n",
    "#    #t.amount_lvl =  t.amount_lvl.astype(str)\n",
    "#\n",
    "#    print('...............................')\n",
    "#    print(f' primary key: {i}')\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_yousfi'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_matrix_graph_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#\n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_matrix_binning'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()\n",
    "#\n",
    "#    \n",
    "#    ax = sns.scatterplot(t, \n",
    "#                    x='date', \n",
    "#                    y='amount', \n",
    "#                    style='is_rec_orii',\n",
    "#                    hue='cluster_id_dbscan'\n",
    "#                   )\n",
    "#    ax.set(xticklabels=[]) \n",
    "#    ax.set(xlabel=None)\n",
    "#    ax.tick_params(bottom=False)\n",
    "#    sns.move_legend( ax,\"center left\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2dede-f7a8-4fb4-96c7-67d4311d99b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b524f-3e95-4143-9644-f422aaaa2af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b009c1d-2857-499a-82c2-2081095341b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
